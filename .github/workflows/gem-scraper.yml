name: Matrix WARN Scraper with Slack Alerts

on:
  schedule:
    - cron: '0 15 * * *' # 10:00 AM EST
  workflow_dispatch:

permissions:
  contents: write

jobs:
  scrape:
    name: Scrape
    runs-on: ubuntu-latest
    continue-on-error: true
    strategy:
      fail-fast: false
      max-parallel: 15 # Speeds up execution by running more states at once
      matrix:
        # TESTING: Use [ca, ny, ia] for your first run
        # state: [ny, ia]
        state: [al, ak, az, ca, co, ct, de, fl, ga, hi, id, il, in, ia, ks, me, md, mo, mt, ne, nj, nm, ny, ok, or, pa, ri, sc, sd, tn, tx, ut, vt, va, wa, wi]
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Install Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install scraper
        run: pip install warn-scraper

      - name: Scrape
        timeout-minutes: 10 # Prevents a single state from hanging the whole run
        run: |
          mkdir -p ./data/
          python -c "from warn.cli import main; import sys; sys.argv=['warn-scraper', '${{ matrix.state }}', '--data-dir', './data/']; main()"

      - name: upload-artifact
        uses: actions/upload-artifact@v4
        with:
          name: ${{ matrix.state }}
          path: ./data/${{ matrix.state }}.csv

  commit:
    name: Commit and Alert
    runs-on: ubuntu-latest
    needs: scrape
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Fetch all history so rebase works correctly

      - name: Install Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install Dependencies
        run: pip install pandas requests

      - name: Download artifact
        uses: actions/download-artifact@v4
        with:
          pattern: '*'
          path: artifacts/

      - name: Move and Prepare
        run: |
          mkdir -p data
          # Store the 'old' version of the files before overwriting them
          mkdir -p old_data
          cp data/*.csv old_data/ 2>/dev/null || true
          mv artifacts/**/*.csv data/

      - name: Create Standardized Data and Alerts
        env:
          SLACK_BOT_TOKEN: ${{ secrets.SLACK_BOT_TOKEN }}
          SLACK_CHANNEL_ID: ${{ secrets.SLACK_CHANNEL_ID }}
        shell: python
        run: |
          import pandas as pd
          import glob
          import os
          import requests
          from datetime import datetime, timedelta

          token = os.environ.get('SLACK_BOT_TOKEN')
          channel_id = os.environ.get('SLACK_CHANNEL_ID')
          now = datetime.now()
          threshold = now - timedelta(days=30)
          
          URL_MAP = {
              "NY": "https://dol.ny.gov/warn-notices",
              "CA": "https://edd.ca.gov/en/Jobs_and_Training/Layoff_Services_WARN",
              "IA": "https://www.iowaworkforcedevelopment.gov/worker-adjustment-and-retraining-notification-warn-notices",
              "TX": "https://www.twc.texas.gov/biz/warn/warn-notices.html",
              "NJ": "https://www.nj.gov/labor/wagehour/jobloss/warn/warn_index.shtml",
              "SC": "https://scworks.org/employer/employer-resources/warn-notices"
          }

          def get_col(df, keywords):
              for col in df.columns:
                  if any(k in col.lower() for k in keywords):
                      return col
              return None

          def clean_jobs(val):
              try:
                  num = float(str(val).replace(',', '').strip())
                  # Reject if it looks like a date (e.g. 20260105)
                  if num > 1000000: return "N/A"
                  return int(num)
              except:
                  return "N/A"

          def standardize_df(df, state_code):
              # Prioritize specific "Notice" columns to avoid future layoff dates
              notice_col = get_col(df, ['notice date', 'notice_date', 'date of notice', 'received date', 'date_received'])
              if not notice_col: notice_col = get_col(df, ['date'])
              
              co_col = get_col(df, ['company name', 'employer', 'business legal'])
              if not co_col: co_col = get_col(df, ['company'])
              
              jobs_col = get_col(df, ['number of employees', 'num_employees', 'jobs affected', 'total layoff', 'workers affected'])
              if not jobs_col: jobs_col = get_col(df, ['jobs', 'affected'])

              new_df = pd.DataFrame()
              new_df['State'] = [state_code.upper()] * len(df)
              new_df['Company'] = df[co_col].fillna("Unknown") if co_col else "Unknown"
              new_df['Notice Date Raw'] = df[notice_col].fillna("N/A") if notice_col else "N/A"
              new_df['Jobs Affected Raw'] = df[jobs_col].fillna("N/A") if jobs_col else "N/A"
              
              # Clean data
              new_df['Jobs Affected'] = new_df['Jobs Affected Raw'].apply(clean_jobs)
              new_df['Notice Date'] = pd.to_datetime(new_df['Notice Date Raw'], errors='coerce')
              
              # Filter out obviously wrong dates (> 1 year in future or < 2010)
              valid_date_mask = (new_df['Notice Date'].dt.year == 2026) | (new_df['Notice Date'].dt.year == 2025)
              new_df.loc[~valid_date_mask, 'Notice Date'] = pd.NaT
              
              return new_df[['State', 'Company', 'Notice Date', 'Jobs Affected', 'Notice Date Raw']]

          all_standardized = []
          new_for_slack = []

          for f in sorted(glob.glob("./data/*.csv")):
              if "recent_notices.csv" in f: continue
              state = os.path.basename(f).split(".")[0].upper()
              try:
                  df_raw = pd.read_csv(f)
                  df_clean = standardize_df(df_raw, state)
                  all_standardized.append(df_clean)
                  
                  # Alerting logic
                  old_file = f.replace('./data/', './old_data/')
                  if os.path.exists(old_file):
                      df_old = standardize_df(pd.read_csv(old_file), state)
                      new_rows = df_clean[~df_clean['Company'].isin(df_old['Company'])]
                  else:
                      new_rows = df_clean.head(1)

                  for _, row in new_rows.iterrows():
                      if str(row['Company']).lower() in ['unknown', 'nan', '']: continue
                      if pd.isna(row['Notice Date']): continue # Only alert on valid 2025/2026 dates
                      new_for_slack.append(row.to_dict())
              except: continue

          # Save Standardized File
          if all_standardized:
              full_df = pd.concat(all_standardized)
              recent = full_df[full_df['Notice Date'] >= threshold]
              recent = recent.sort_values(by='Notice Date', ascending=False)
              recent['Notice Date'] = recent['Notice Date'].dt.strftime('%Y-%m-%d')
              recent.drop(columns=['Notice Date Raw']).to_csv("./data/recent_notices.csv", index=False)

          # Threaded Slack Alerts
          if token and channel_id and new_for_slack:
              header = f"*Daily WARN Report: {now.strftime('%Y-%m-%d')}*"
              resp = requests.post("https://slack.com/api/chat.postMessage", headers={"Authorization": f"Bearer {token}"}, 
                                   json={"channel": channel_id, "text": header}).json()
              ts = resp.get("ts")
              for n in new_for_slack:
                  s_url = URL_MAP.get(n['State'], f"https://www.google.com/search?q={n['State']}+WARN+notices")
                  date_str = n['Notice Date'].strftime('%Y-%m-%d') if pd.notna(n['Notice Date']) else n['Notice Date Raw']
                  payload = {
                      "channel": channel_id, "thread_ts": ts,
                      "blocks": [
                          {"type": "section", "text": {"type": "mrkdwn", "text": f"*New WARN Notice: {n['State']}*\n*Company:* {n['Company']}\n*Jobs affected:* {n['Jobs Affected']}\n*Notice Date:* {date_str}"}},
                          {"type": "actions", "elements": [{"type": "button", "text": {"type": "plain_text", "text": "ðŸ”— View Official Page"}, "url": s_url}]}
                      ]
                  }
                  requests.post("https://slack.com/api/chat.postMessage", headers={"Authorization": f"Bearer {token}"}, json=payload)

      - name: Update Index Page
        shell: python
        run: |
          import glob, os
          from datetime import datetime, timedelta
          # Convert UTC to EST (-5 hours)
          est_time = datetime.utcnow() - timedelta(hours=5)
          content = "# WARN Act Data Portal\n\nAutomated tracking of layoff notices across the U.S.\n\n"
          content += f"### Latest Global Update: {est_time.strftime('%Y-%m-%d %I:%M %p')} EST\n\n"
          content += "| State | Data | Source |\n| :--- | :--- | :--- |\n"
          if os.path.exists("./data/recent_notices.csv"):
              content += "| **All States** | [Download Standardized 30-Day CSV](./data/recent_notices.csv) | - |\n"
          for f in sorted(glob.glob("./data/*.csv")):
              if "recent_notices.csv" in f: continue
              st = os.path.basename(f).replace('.csv', '').upper()
              content += f"| {st} | [Raw CSV](./data/{st.lower()}.csv) | [Official](https://www.google.com/search?q={st}+WARN) |\n"
          with open("README.md", "w") as f: f.write(content)

      - name: Commit and push
        run: |
          git config user.name "GitHub Actions"
          git config user.email "actions@users.noreply.github.com"
          git add ./data/ README.md
          if git diff --staged --quiet; then
            echo "No changes detected."
          else
            git commit -m "Update standardized WARN data [skip ci]"
            git pull --rebase origin main
            git push origin main
          fi
